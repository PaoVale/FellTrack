{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PaoVale/FellTrack/blob/main/FeelTrack.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "JT5o7bm5gmyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup and Import Libraries\n",
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Download nltk duct utils\n",
        "nltk.data.path.append('/usr/share/nltk_data')\n",
        "nltk.download('stopwords', '/usr/share/nltk_data')\n",
        "nltk.download('wordnet', '/usr/share/nltk_data')\n",
        "nltk.download('punkt', '/usr/share/nltk_data')\n",
        "nltk.download('punkt_tab', download_dir='/usr/share/nltk_data')\n",
        "!yes | unzip -q /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/\n",
        "\n",
        "# Setting the style for seaborn\n",
        "sns.set(style='whitegrid')\n",
        "\n",
        "# To suppress warnings\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set verbosity during model trainig\n",
        "verbose = True"
      ],
      "metadata": {
        "id": "ZXiRlFfDZx4G",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-04T22:28:15.897016Z",
          "iopub.execute_input": "2025-02-04T22:28:15.897535Z",
          "iopub.status.idle": "2025-02-04T22:28:16.493364Z",
          "shell.execute_reply.started": "2025-02-04T22:28:15.897502Z",
          "shell.execute_reply": "2025-02-04T22:28:16.491970Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "Lvw536GCgmyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "tKwOqdw-gmyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load training dataset part 1\n",
        "train_pt1 = pd.read_csv('Train.csv')\n",
        "\n",
        "# Drop not used columns\n",
        "train_pt1 = train_pt1.drop(columns=['Id'], axis=1)\n",
        "\n",
        "# Load training dataset part 1\n",
        "train_pt2 = pd.read_csv('Twitter_Data.csv')\n",
        "\n",
        "# Rename columns\n",
        "train_pt2 = train_pt2.rename(columns={\"clean_text\": \"Body\", \"category\": \"Sentiment Type\"})\n",
        "\n",
        "# Replace values to match lables of part 1\n",
        "train_pt2 = train_pt2.replace(-1, \"negative\")\n",
        "train_pt2 = train_pt2.replace(0, \"neutral\")\n",
        "train_pt2 = train_pt2.replace(1, \"positive\")\n",
        "\n",
        "# Concat the two parts to get the final training set\n",
        "train = pd.concat([train_pt1, train_pt2], ignore_index=True, sort=False).reset_index(drop=True)\n",
        "\n",
        "\n",
        "# Load test dataset\n",
        "test  = pd.read_csv('synthetic_social_media_data.csv')"
      ],
      "metadata": {
        "id": "nkJiCpKLZ7p8",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-04T22:28:16.495630Z",
          "iopub.execute_input": "2025-02-04T22:28:16.496098Z",
          "iopub.status.idle": "2025-02-04T22:28:17.282369Z",
          "shell.execute_reply.started": "2025-02-04T22:28:16.496047Z",
          "shell.execute_reply": "2025-02-04T22:28:17.281279Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Display first few rows of training set\n",
        "print(\"Training Dataset:\")\n",
        "print(train.head())"
      ],
      "metadata": {
        "id": "X_FXt1roalc5",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-04T22:28:17.284689Z",
          "iopub.execute_input": "2025-02-04T22:28:17.285143Z",
          "iopub.status.idle": "2025-02-04T22:28:17.294288Z",
          "shell.execute_reply.started": "2025-02-04T22:28:17.285097Z",
          "shell.execute_reply": "2025-02-04T22:28:17.293038Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Display first few rows of test set\n",
        "print(\"\\nTest Dataset:\")\n",
        "print(test.head())"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-04T22:28:17.295829Z",
          "iopub.execute_input": "2025-02-04T22:28:17.296184Z",
          "iopub.status.idle": "2025-02-04T22:28:17.316802Z",
          "shell.execute_reply.started": "2025-02-04T22:28:17.296156Z",
          "shell.execute_reply": "2025-02-04T22:28:17.315470Z"
        },
        "id": "FWIrBZp_gmyJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values in Training Dataset\n",
        "train.isnull().sum()"
      ],
      "metadata": {
        "id": "yZ2Fbp89apB4",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-04T22:28:17.318220Z",
          "iopub.execute_input": "2025-02-04T22:28:17.318658Z",
          "iopub.status.idle": "2025-02-04T22:28:17.363949Z",
          "shell.execute_reply.started": "2025-02-04T22:28:17.318625Z",
          "shell.execute_reply": "2025-02-04T22:28:17.362773Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Class distribution\n",
        "train['Sentiment Type'].value_counts()"
      ],
      "metadata": {
        "id": "PvxDN6f6ask1",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-04T22:28:17.365217Z",
          "iopub.execute_input": "2025-02-04T22:28:17.365551Z",
          "iopub.status.idle": "2025-02-04T22:28:17.395592Z",
          "shell.execute_reply.started": "2025-02-04T22:28:17.365517Z",
          "shell.execute_reply": "2025-02-04T22:28:17.394611Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Preprocessing"
      ],
      "metadata": {
        "id": "Qsej8noxgmyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows where any column has missing values\n",
        "train = train.dropna()\n",
        "test = test.dropna()"
      ],
      "metadata": {
        "id": "_s_fhcMdaxnP",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-04T22:28:17.396796Z",
          "iopub.execute_input": "2025-02-04T22:28:17.397276Z",
          "iopub.status.idle": "2025-02-04T22:28:17.450796Z",
          "shell.execute_reply.started": "2025-02-04T22:28:17.397231Z",
          "shell.execute_reply": "2025-02-04T22:28:17.449509Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting class distribution\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(data=train, x='Sentiment Type', palette='viridis')\n",
        "plt.title('Sentiment Type Distribution')\n",
        "plt.xlabel('Sentiment Type')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yZbnJjBqau2t",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-04T22:28:17.453465Z",
          "iopub.execute_input": "2025-02-04T22:28:17.453833Z",
          "iopub.status.idle": "2025-02-04T22:28:17.757625Z",
          "shell.execute_reply.started": "2025-02-04T22:28:17.453803Z",
          "shell.execute_reply": "2025-02-04T22:28:17.756507Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove contractions\n",
        "def contractions(s):\n",
        "    s = re.sub(r\"won't\", \"will not\", s, flags=re.IGNORECASE)\n",
        "    s = re.sub(r\"wouldn't\", \"would not\", s, flags=re.IGNORECASE)\n",
        "    s = re.sub(r\"couldn't\", \"could not\", s, flags=re.IGNORECASE)\n",
        "    s = re.sub(r\"\\’d\", \" would\", s, flags=re.IGNORECASE)\n",
        "    s = re.sub(r\"can't\", \"can not\", s, flags=re.IGNORECASE)\n",
        "    s = re.sub(r\"n’t\", \" not\", s, flags=re.IGNORECASE)\n",
        "    s = re.sub(r\"\\’re\", \" are\", s, flags=re.IGNORECASE)\n",
        "    s = re.sub(r\"\\’s\", \" is\", s, flags=re.IGNORECASE)\n",
        "    s = re.sub(r\"\\’ll\", \" will\", s, flags=re.IGNORECASE)\n",
        "    s = re.sub(r\"\\’t\", \" not\", s, flags=re.IGNORECASE)\n",
        "    s = re.sub(r\"\\’ve\", \" have\", s, flags=re.IGNORECASE)\n",
        "    s = re.sub(r\"\\’m\", \" am\", s, flags=re.IGNORECASE)\n",
        "    return s\n",
        "\n",
        "# Apply the function\n",
        "train['Body'] = train['Body'].apply(lambda x: contractions(x))\n",
        "test['Post Content'] = test['Post Content'].apply(lambda x: contractions(x))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-04T22:28:17.759297Z",
          "iopub.execute_input": "2025-02-04T22:28:17.759637Z",
          "iopub.status.idle": "2025-02-04T22:28:21.579723Z",
          "shell.execute_reply.started": "2025-02-04T22:28:17.759609Z",
          "shell.execute_reply": "2025-02-04T22:28:21.578747Z"
        },
        "id": "qykyIg_fgmyL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove non-alpha characters\n",
        "train['Body'] = train['Body'].apply(lambda x: \" \".join([re.sub('[^A-Za-z]+','', x) for x in nltk.word_tokenize(x)]))\n",
        "test['Post Content'] = test['Post Content'].apply(lambda x: \" \".join([re.sub('[^A-Za-z]+','', x) for x in nltk.word_tokenize(x)]))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-04T22:28:21.580744Z",
          "iopub.execute_input": "2025-02-04T22:28:21.581130Z",
          "iopub.status.idle": "2025-02-04T22:28:51.577964Z",
          "shell.execute_reply.started": "2025-02-04T22:28:21.581082Z",
          "shell.execute_reply": "2025-02-04T22:28:51.576661Z"
        },
        "id": "vYE_ID2ugmyL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the extra spaces between the words\n",
        "train['Body'] = train['Body'].apply(lambda x: re.sub(' +', ' ', x))\n",
        "test['Post Content'] = test['Post Content'].apply(lambda x: re.sub(' +', ' ', x))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-04T22:28:51.579189Z",
          "iopub.execute_input": "2025-02-04T22:28:51.579541Z",
          "iopub.status.idle": "2025-02-04T22:28:52.918292Z",
          "shell.execute_reply.started": "2025-02-04T22:28:51.579511Z",
          "shell.execute_reply": "2025-02-04T22:28:52.917149Z"
        },
        "id": "XT2OU6-OgmyM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the stop words by using the NLTK package\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop = stopwords.words('english')\n",
        "train['Body'] = train['Body'].apply(lambda x: \" \".join([x for x in x.split() if x not in stop]))\n",
        "test['Post Content'] = test['Post Content'].apply(lambda x: \" \".join([x for x in x.split() if x not in stop]))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-04T22:28:52.919412Z",
          "iopub.execute_input": "2025-02-04T22:28:52.919782Z",
          "iopub.status.idle": "2025-02-04T22:29:00.950698Z",
          "shell.execute_reply.started": "2025-02-04T22:28:52.919738Z",
          "shell.execute_reply": "2025-02-04T22:29:00.949332Z"
        },
        "id": "tR49G14RgmyM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform lemmatization using the wordnet lemmatizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "train['Body'] = train['Body'].apply(lambda x: \" \".join([lemmatizer.lemmatize(w) for w in nltk.word_tokenize(x)]))\n",
        "test['Post Content'] = test['Post Content'].apply(lambda x: \" \".join([lemmatizer.lemmatize(w) for w in nltk.word_tokenize(x)]))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-04T22:29:00.951559Z",
          "iopub.execute_input": "2025-02-04T22:29:00.951898Z",
          "iopub.status.idle": "2025-02-04T22:29:34.462538Z",
          "shell.execute_reply.started": "2025-02-04T22:29:00.951867Z",
          "shell.execute_reply": "2025-02-04T22:29:34.460925Z"
        },
        "id": "JLmQS-7bgmyM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividi il dataset in training e test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = train['Body']\n",
        "y = train['Sentiment Type']\n",
        "\n",
        "# Dividiamo i dati in training e test set (80% training, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-04T22:29:34.463780Z",
          "iopub.execute_input": "2025-02-04T22:29:34.464120Z",
          "iopub.status.idle": "2025-02-04T22:29:34.562340Z",
          "shell.execute_reply.started": "2025-02-04T22:29:34.464089Z",
          "shell.execute_reply": "2025-02-04T22:29:34.561104Z"
        },
        "id": "LOMdA0ZRgmyM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Extraction using TF-IDF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Init the TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=6000, ngram_range=(1, 2))\n",
        "\n",
        "# Apply TF-IDF to vectorize the text\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "print(\"\\nShape of X_train_tfidf:\", X_train_tfidf.shape)\n",
        "print(\"Shape of X_test_tfidf:\", X_test_tfidf.shape)"
      ],
      "metadata": {
        "id": "pTo1s8_-R0lx",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-04T22:29:34.563677Z",
          "iopub.execute_input": "2025-02-04T22:29:34.564001Z",
          "iopub.status.idle": "2025-02-04T22:29:45.494446Z",
          "shell.execute_reply.started": "2025-02-04T22:29:34.563971Z",
          "shell.execute_reply": "2025-02-04T22:29:45.493262Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models Training"
      ],
      "metadata": {
        "id": "xpNcn1gvgmyM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Class Weights"
      ],
      "metadata": {
        "id": "bquYN-6ZgmyN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute class weight to contrast class imbalance\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "classes = np.unique(y_train)\n",
        "class_weight = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
        "class_weight = {label: weight for label, weight in zip(classes, class_weight)}\n",
        "\n",
        "print(\"Class Weight\")\n",
        "print(class_weight)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-04T22:29:45.495638Z",
          "iopub.execute_input": "2025-02-04T22:29:45.495986Z",
          "iopub.status.idle": "2025-02-04T22:29:45.616461Z",
          "shell.execute_reply.started": "2025-02-04T22:29:45.495957Z",
          "shell.execute_reply": "2025-02-04T22:29:45.614989Z"
        },
        "id": "LhiC8uC1gmyN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression"
      ],
      "metadata": {
        "id": "QMxu19T-Q4bx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the LogisticRegression class\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Logistic Regression\n",
        "logistic_model = LogisticRegression(max_iter=1000, class_weight=class_weight, n_jobs=-1, random_state=42, verbose=verbose)\n",
        "\n",
        "# Fit the model on training data\n",
        "logistic_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_test_pred_logistic = logistic_model.predict(X_test_tfidf)\n",
        "\n",
        "# Logistic Regression Results\n",
        "print(\"\\n[Logistic Regression]\")\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, y_test_pred_logistic))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_test_pred_logistic))\n",
        "\n",
        "# Show the heatmap\n",
        "sns.heatmap(confusion_matrix(y_test, y_test_pred_logistic), annot=True, fmt='d', cmap='Blues')\n",
        "plt.title(\"Confusion Matrix - Logistic Regression\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EDoao8mWbDAs",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-04T22:29:45.617419Z",
          "iopub.execute_input": "2025-02-04T22:29:45.617780Z",
          "iopub.status.idle": "2025-02-04T22:30:03.446242Z",
          "shell.execute_reply.started": "2025-02-04T22:29:45.617751Z",
          "shell.execute_reply": "2025-02-04T22:30:03.444617Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forest"
      ],
      "metadata": {
        "id": "tv6zvmBxQ_DF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the RandomForestClassifier class\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Random Forest\n",
        "rf_model = RandomForestClassifier(n_estimators=256, max_depth=96, class_weight=class_weight, n_jobs=-1, random_state=42, verbose=verbose)\n",
        "\n",
        "# Fit the model on training data\n",
        "rf_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_test_pred_rf = rf_model.predict(X_test_tfidf)\n",
        "\n",
        "# Random Forest Results\n",
        "print(\"\\n[Random Forest]\")\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, y_test_pred_rf))\n",
        "print(\"Classification Report (Test Data):\\n\", classification_report(y_test, y_test_pred_rf))\n",
        "\n",
        "# Show the heatmap\n",
        "sns.heatmap(confusion_matrix(y_test, y_test_pred_rf), annot=True, fmt='d', cmap='Greens')\n",
        "plt.title(\"Confusion Matrix - Random Forest (Test Data)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1S_oOJ_gOyhe",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-04T22:30:03.448227Z",
          "iopub.execute_input": "2025-02-04T22:30:03.448849Z",
          "iopub.status.idle": "2025-02-04T22:32:03.766172Z",
          "shell.execute_reply.started": "2025-02-04T22:30:03.448790Z",
          "shell.execute_reply": "2025-02-04T22:32:03.764989Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SVC"
      ],
      "metadata": {
        "id": "XHUbFFYBpKXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the LinearSVC class\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "# SVC with a Linear kernel (SVC(kernel=\"linear\"))\n",
        "svm_model = LinearSVC(max_iter=1000, tol=0.001, class_weight=class_weight, random_state=42, verbose=verbose)\n",
        "\n",
        "# Fit the model on training data\n",
        "svm_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred_svm = svm_model.predict(X_test_tfidf)\n",
        "\n",
        "# SVM Results\n",
        "print(\"\\n[SVM]\")\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
        "print(\"Classification Report: \\n\", classification_report(y_test, y_pred_svm))\n",
        "\n",
        "# Show the heatmap\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred_svm), annot=True, fmt='d', cmap='Blues')\n",
        "plt.title(\"Confusion Matrix SVM\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Oy4a5CsupSL1",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-04T22:32:03.767402Z",
          "iopub.execute_input": "2025-02-04T22:32:03.767830Z",
          "iopub.status.idle": "2025-02-04T22:32:10.549587Z",
          "shell.execute_reply.started": "2025-02-04T22:32:03.767795Z",
          "shell.execute_reply": "2025-02-04T22:32:10.548466Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MLP"
      ],
      "metadata": {
        "id": "3DwdwXhbTICc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#errore#correggi\n",
        "# Importing the MLPClassifier class\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "\n",
        "\n",
        "# MLP Classifier\n",
        "mlp_model = MLPClassifier(hidden_layer_sizes=256, max_iter=500, learning_rate_init=0.001, early_stopping=True, random_state=42, verbose=verbose)\n",
        "\n",
        "# Fit the model on training data\n",
        "mlp_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_test_pred_mlp = mlp_model.predict(X_test_tfidf)\n",
        "\n",
        "# MLP Classifier Results\n",
        "print(\"\\n[MLP Classifier]\")\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, y_test_pred_mlp))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_test_pred_mlp))\n",
        "\n",
        "# Show the heatmap\n",
        "sns.heatmap(confusion_matrix(y_test, y_test_pred_mlp), annot=True, fmt='d', cmap='Blues')\n",
        "plt.title(\"Confusion Matrix - MLP Classifier\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SZOfIAAeyurh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Importing the MLPClassifier class\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Create a LabelEncoder object\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit the LabelEncoder on the target variable\n",
        "label_encoder.fit(y_train)\n",
        "\n",
        "# Transform the target variable to numerical labels\n",
        "y_train_encoded = label_encoder.transform(y_train)\n",
        "\n",
        "# MLP Classifier\n",
        "mlp_model = MLPClassifier(hidden_layer_sizes=256, max_iter=500, learning_rate_init=0.001, early_stopping=True, random_state=42, verbose=verbose)\n",
        "\n",
        "# Fit the model on training data with encoded target variable\n",
        "mlp_model.fit(X_train_tfidf, y_train_encoded)\n",
        "\n",
        "# Predict on test data (remember to encode y_test as well)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "y_test_pred_mlp = mlp_model.predict(X_test_tfidf)\n",
        "\n",
        "# Convert predictions back to original labels\n",
        "y_test_pred_mlp_labels = label_encoder.inverse_transform(y_test_pred_mlp)\n",
        "\n",
        "# MLP Classifier Results\n",
        "print(\"\\n[MLP Classifier]\")\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, y_test_pred_mlp_labels))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_test_pred_mlp_labels))\n",
        "\n",
        "# Show the heatmap (use y_test_pred_mlp_labels)\n",
        "sns.heatmap(confusion_matrix(y_test, y_test_pred_mlp_labels), annot=True, fmt='d', cmap='Blues')\n",
        "plt.title(\"Confusion Matrix - MLP Classifier\")\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Ga454j2-sI3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Statistics"
      ],
      "metadata": {
        "id": "kX5mSH5elHg9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Number of Likes vs Sentiment Labels"
      ],
      "metadata": {
        "id": "QbsAQ3x7JAta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x='Sentiment Label', y='Number of Likes', data=test, palette='coolwarm')\n",
        "plt.title('Number of Likes by Sentiment Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ASrLZQHTIyiC",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-04T22:35:43.666708Z",
          "iopub.execute_input": "2025-02-04T22:35:43.667081Z",
          "iopub.status.idle": "2025-02-04T22:35:43.920574Z",
          "shell.execute_reply.started": "2025-02-04T22:35:43.667051Z",
          "shell.execute_reply": "2025-02-04T22:35:43.919524Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Relationship between Number of Likes, Shares, and Comments"
      ],
      "metadata": {
        "id": "RWV1P26uJF9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.pairplot(test[['Number of Likes', 'Number of Shares', 'Number of Comments']], diag_kind='kde', palette='coolwarm')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "c1I-DuXjJG-g",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-04T22:35:43.925091Z",
          "iopub.execute_input": "2025-02-04T22:35:43.925538Z",
          "iopub.status.idle": "2025-02-04T22:35:47.156161Z",
          "shell.execute_reply.started": "2025-02-04T22:35:43.925468Z",
          "shell.execute_reply": "2025-02-04T22:35:47.154894Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Likes vs Follower Count for Different Post Types\n",
        "\n"
      ],
      "metadata": {
        "id": "U8gx9ZUIJXGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x='User Follower Count', y='Number of Likes', hue='Post Type', data=test, palette='coolwarm', alpha=0.6)\n",
        "plt.title('Number of Likes vs Follower Count by Post Type')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GOnPrM8OJWWr",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-04T22:35:47.157805Z",
          "iopub.execute_input": "2025-02-04T22:35:47.158191Z",
          "iopub.status.idle": "2025-02-04T22:35:47.716916Z",
          "shell.execute_reply.started": "2025-02-04T22:35:47.158157Z",
          "shell.execute_reply": "2025-02-04T22:35:47.715557Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Post Type Distribution by Sentiment Label"
      ],
      "metadata": {
        "id": "22MMGptJJh9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(x='Sentiment Label', hue='Post Type', data=test, palette='coolwarm')\n",
        "plt.title('Distribution of Post Types by Sentiment Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YGUS8IJsJinK",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-04T22:35:47.718099Z",
          "iopub.execute_input": "2025-02-04T22:35:47.718511Z",
          "iopub.status.idle": "2025-02-04T22:35:47.990028Z",
          "shell.execute_reply.started": "2025-02-04T22:35:47.718453Z",
          "shell.execute_reply": "2025-02-04T22:35:47.988838Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Heatmap of Correlations between Numeric Variables"
      ],
      "metadata": {
        "id": "qWAjxzQZJ0Vv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "corr = test[['Number of Likes', 'Number of Shares', 'Number of Comments', 'User Follower Count']].corr()\n",
        "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
        "plt.title('Correlation Matrix of Engagement Metrics')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "diuU6ONJJ0-J",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-04T22:35:47.991236Z",
          "iopub.execute_input": "2025-02-04T22:35:47.991654Z",
          "iopub.status.idle": "2025-02-04T22:35:48.341266Z",
          "shell.execute_reply.started": "2025-02-04T22:35:47.991623Z",
          "shell.execute_reply": "2025-02-04T22:35:48.340130Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Assumiamo che i tuoi dati siano in un DataFrame Pandas chiamato 'df'\n",
        "# Con colonne: 'sentiment', 'likes', 'shares', 'comments'\n",
        "\n",
        "# Selezionare le colonne rilevanti\n",
        "df_filtered = test[['Sentiment Label', 'Number of Likes', 'Number of Shares', 'Number of Comments']]\n",
        "\n",
        "# Creare un grafico a barre raggruppate\n",
        "df_filtered.groupby('Sentiment Label').sum().plot(kind='bar')\n",
        "plt.title('Relazione tra Sentiment e Interazioni')\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Numero di Interazioni')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tkgnc6I4BCAU",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-04T22:35:48.342468Z",
          "iopub.execute_input": "2025-02-04T22:35:48.342855Z",
          "iopub.status.idle": "2025-02-04T22:35:48.621771Z",
          "shell.execute_reply.started": "2025-02-04T22:35:48.342813Z",
          "shell.execute_reply": "2025-02-04T22:35:48.620663Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression - Feature Importance\n",
        "if 'logistic_model' in globals() and isinstance(logistic_model, LogisticRegression):\n",
        "    # Get feature names from the TF-IDF vectorizer\n",
        "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "    coefficients = logistic_model.coef_[0]\n",
        "\n",
        "    # Create a DataFrame to hold feature names and their importance\n",
        "    feature_importance_logistic = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Importance': np.abs(coefficients)\n",
        "    })\n",
        "\n",
        "    # Sort by importance\n",
        "    feature_importance_logistic = feature_importance_logistic.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "    # Select the top 20 features\n",
        "    top_features_logistic = feature_importance_logistic.head(20)\n",
        "\n",
        "    # Plot important features for Logistic Regression\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.barh(top_features_logistic['Feature'], top_features_logistic['Importance'], color='skyblue')\n",
        "    plt.xlabel('Importance')\n",
        "    plt.title('Top 20 Features by Importance (Logistic Regression)')\n",
        "    plt.gca().invert_yaxis()  # To display the highest importance at the top\n",
        "    plt.show()\n",
        "\n",
        "# Random Forest - Feature Importance\n",
        "if 'rf_model' in globals() and isinstance(rf_model, RandomForestClassifier):\n",
        "    # Get feature names from the TF-IDF vectorizer\n",
        "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "    # Get feature importance from the Random Forest model\n",
        "    feature_importance_rf = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Importance': rf_model.feature_importances_\n",
        "    })\n",
        "\n",
        "    # Sort by importance\n",
        "    feature_importance_rf = feature_importance_rf.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "    # Select the top 20 features\n",
        "    top_features_rf = feature_importance_rf.head(20)\n",
        "\n",
        "    # Plot important features for Random Forest\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.barh(top_features_rf['Feature'], top_features_rf['Importance'], color='green')\n",
        "    plt.xlabel('Importance')\n",
        "    plt.title('Top 20 Features by Importance (Random Forest)')\n",
        "    plt.gca().invert_yaxis()  # To display the highest importance at the top\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "9ejsVoDRbRIS",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-04T22:35:48.623017Z",
          "iopub.execute_input": "2025-02-04T22:35:48.623428Z",
          "iopub.status.idle": "2025-02-04T22:35:49.548599Z",
          "shell.execute_reply.started": "2025-02-04T22:35:48.623384Z",
          "shell.execute_reply": "2025-02-04T22:35:49.547363Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#Salva tutto in unico file\n",
        "y_test_pred = logistic_model.predict(tfidf_vectorizer.transform(test['Post Content']))\n",
        "\n",
        "test_results = test.copy()\n",
        "test_results['Sentiment Type'] = y_test_pred\n",
        "\n",
        "test_results[['Post ID', 'Post Content', 'Sentiment Type']].to_csv('test_predictions_logistic.csv', index=False)\n",
        "\n",
        "\n",
        "y_test_pred_rf = rf_model.predict(tfidf_vectorizer.transform(test['Post Content']))\n",
        "\n",
        "test_results = test.copy()\n",
        "test_results['Sentiment Type'] = y_test_pred_rf\n",
        "\n",
        "test_results[['Post ID', 'Post Content', 'Sentiment Type']].to_csv('test_predictions_rf.csv', index=False)"
      ],
      "metadata": {
        "id": "eiLvNwxobSEq",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-04T22:35:49.549928Z",
          "iopub.execute_input": "2025-02-04T22:35:49.550339Z",
          "iopub.status.idle": "2025-02-04T22:35:49.977854Z",
          "shell.execute_reply.started": "2025-02-04T22:35:49.550294Z",
          "shell.execute_reply": "2025-02-04T22:35:49.976590Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "# model = trained logistic regression model\n",
        "# tfidf = trained TF-IDF vectorizer\n",
        "\n",
        "# Save the logistic model and the TF-IDF vectorizer\n",
        "joblib.dump(logistic_model, 'sentiment_modelLogistic.pkl')\n",
        "joblib.dump(tfidf_vectorizer, 'tfidf_vectorizeLogisticr.pkl')\n",
        "\n",
        "\n",
        "# Save the random forest model and the TF-IDF vectorizer\n",
        "joblib.dump(rf_model, 'sentiment_model_randomForest.pkl')\n",
        "joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer_randomForest.pkl')"
      ],
      "metadata": {
        "id": "R27isWtstDuM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}